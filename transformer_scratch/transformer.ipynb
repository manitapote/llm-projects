{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0d1a04-2fbd-40b8-9b84-cfd737577fa1",
   "metadata": {},
   "source": [
    "#### **This notebook implements the Transformer Encoder Architecture from scratch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e64b7a5-4dc3-4bb2-af1a-468076f4807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781acff0-2d66-450f-b95a-7c8d7479f0d2",
   "metadata": {},
   "source": [
    "### **Input and positional encoding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b52373f-87a7-4191-96c4-31dfde449e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddingBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embed_dim, \n",
    "                 max_seq_len\n",
    "                ):\n",
    "        super().__init__()\n",
    "        #this holds the embedding of all the vocab\n",
    "        #this is used for the lookup in the forward stage\n",
    "        self.token_embeding = nn.Embedding(vocab_size,\n",
    "                                           embed_dim\n",
    "                                          )\n",
    "        print('Maximum vocab size of the entire system: ',\n",
    "              vocab_size\n",
    "             )\n",
    "        print('Embedding of each token: ', \n",
    "              embed_dim\n",
    "             )\n",
    "        print('Maximum sequence length: ',\n",
    "              max_seq_len\n",
    "             )\n",
    "        \n",
    "        self.positional_embedding = self.get_positional_encoding(\n",
    "            max_seq_len,\n",
    "            embed_dim\n",
    "        )\n",
    "\n",
    "    def get_positional_encoding(self, \n",
    "                                max_seq_len, \n",
    "                                d_model\n",
    "                               ):\n",
    "        #max_seq_len is the maximum sequence length\n",
    "        #possible\n",
    "        #d_model is the dimension of the embedding\n",
    "        #rows capture positional information\n",
    "        #column catpures frequency information\n",
    "        pe = torch.zeros(max_seq_len,\n",
    "                         d_model\n",
    "                        )\n",
    "\n",
    "        #positions of words\n",
    "        position = torch.arange(0, \n",
    "                                max_seq_len,\n",
    "                                dtype=torch.float\n",
    "                               ).unsqueeze(1)\n",
    "        print('Total positions to be considered for the positional encoding: ',\n",
    "              d_model\n",
    "             )\n",
    "        \n",
    "        even_indices =torch.arange(0, \n",
    "                                   d_model, \n",
    "                                   2).float()\n",
    "        \n",
    "        div_term = torch.exp(even_indices * (\n",
    "            -math.log(10000.0) / d_model)\n",
    "                            )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        \n",
    "        print('Positional encoding dimension (maximum_sequence_length, size_of_position_encoding): ', \n",
    "              pe.size()\n",
    "             )\n",
    "\n",
    "        \n",
    "        return pe\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x: (batch_size, seq_len)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        #(seq_len, d_model)\n",
    "        token_embedding = self.token_embeding(x)\n",
    "        #(seq_len, d_model)\n",
    "        pos_emb = self.positional_embedding[:seq_len,:]\n",
    "\n",
    "        #output of this is (seq_len, d_model)\n",
    "        return token_embedding + pos_emb, pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121da15d-986f-4458-8929-9305c4758231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2ae4a8-cb37-42c6-84b1-9991afc36506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum vocab size of the entire system:  200\n",
      "Embedding of each token:  100\n",
      "Maximum sequence length:  20\n",
      "Total positions to be considered for the positional encoding:  100\n",
      "Positional encoding dimension (maximum_sequence_length, size_of_position_encoding):  torch.Size([20, 100])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input tokens :  tensor([[187,  86, 183,  51,  61, 184,  11,  19,  74, 156],\n",
      "        [163, 160,  18,  98, 129,  65, 159,  27, 106,  98]])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 200\n",
    "embed_dim = 100\n",
    "max_seq_len = 20\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "model = InputEmbeddingBlock(vocab_size, \n",
    "                            embed_dim,\n",
    "                            max_seq_len\n",
    "                           )\n",
    "input_tokens = torch.randint(0, vocab_size,\n",
    "                          (batch_size, seq_len)\n",
    "                         )\n",
    "#these are input tokens of the sequence\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "print('Input tokens : ', input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89619694-3edc-4212-a96b-6ccc9a7151a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "output, pos_emb = model(input_tokens)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b64001-6594-4139-a4d1-7127ccaf0eb2",
   "metadata": {},
   "source": [
    "#### **Self-Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d53e2f-3d6b-4991-9e19-dd7e630b3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, number_of_head):\n",
    "        super().__init__()\n",
    "        self.embed_dim = d_model\n",
    "        self.number_of_head = int(number_of_head)\n",
    "        self.head_dim = int(self.embed_dim / self.number_of_head)\n",
    "\n",
    "        print('\\n Embedding size: ', self.embed_dim)\n",
    "        print('Number of heads: ', self.number_of_head)\n",
    "        print('Head dimension: ', self.head_dim)\n",
    "        \n",
    "    \n",
    "        #These project to the total dimension across all head.\n",
    "        self.W_q = nn.Linear(self.embed_dim, \n",
    "                             self.embed_dim, \n",
    "                             bias=False\n",
    "                            )\n",
    "        self.W_k = nn.Linear(self.embed_dim,\n",
    "                             self.embed_dim,\n",
    "                             bias=False\n",
    "                            )\n",
    "        self.W_v = nn.Linear(self.embed_dim,\n",
    "                             self.embed_dim,\n",
    "                             bias=False\n",
    "                            )\n",
    "    \n",
    "        self.scale = 1/ math.sqrt(self.head_dim)\n",
    "\n",
    "        self.output_proj = nn.Linear(embed_dim,\n",
    "                                     embed_dim\n",
    "                                    )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "\n",
    "        #This results in batch_size, seq_len,\n",
    "        #embed_dim\n",
    "        Q_all = self.W_q(x) \n",
    "        K_all = self.W_k(x)\n",
    "        V_all = self.W_v(x)\n",
    "\n",
    "        #Split the heads\n",
    "        Q = Q_all.view(batch_size, seq_len, \n",
    "                       self.number_of_head, \n",
    "                       self.head_dim\n",
    "                      )\n",
    "        K = K_all.view(batch_size, seq_len, \n",
    "                       self.number_of_head, \n",
    "                       self.head_dim\n",
    "                      )\n",
    "        V = V_all.view(batch_size, seq_len, \n",
    "                       self.number_of_head, \n",
    "                       self.head_dim\n",
    "                      )\n",
    "\n",
    "        print('Dimension of Q, K and V, (batch_size, seq_len, number of head, head_dim): ',\n",
    "              V.size()\n",
    "             )\n",
    "\n",
    "        #batch_size, number of head,\n",
    "        #seq length, head_dim\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, \n",
    "                              #batch_size, number of head,\n",
    "                              #head_dim, seq_length\n",
    "                              K.transpose(-2, -1)\n",
    "                             )*self.scale\n",
    "        #output of scores = (batch_size, number_of_head,\n",
    "        #seq_length, seq_length\n",
    "\n",
    "        print('Dimension of scores: (batch_size, number of head, seq length, seq length), ', \n",
    "              scores.size()\n",
    "             )\n",
    "        \n",
    "        attention_weights = torch.softmax(scores,\n",
    "                                          dim=-1\n",
    "                                         )\n",
    "        #output of attention_weights = (batch_size,\n",
    "        #number_of_head, seq_length, seq_length)\n",
    "        print('Dimension of attention weights: (batch size, number of head, seq_lenght, seq_length) ',\n",
    "              attention_weights.size()\n",
    "             )\n",
    "\n",
    "        output = torch.matmul(attention_weights,\n",
    "                              V\n",
    "                             )\n",
    "        #(batch_size, number_of_head,\n",
    "        #seq_length, head_dim)\n",
    "        print('Dimension of output (batch_size, number of head, seq length, head_dim) :',\n",
    "              output.size()\n",
    "             )\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size,\n",
    "                             seq_len,\n",
    "                             embed_dim\n",
    "                            )\n",
    "        print('Dimension of output after projection (batch_size, seq_len, embed_dim) :',\n",
    "              output.size()\n",
    "             )\n",
    "\n",
    "        return self.output_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a91b90-9541-4f73-a10b-e2b086012ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cac0cb5-6db3-4ecb-9d49-29d625b18d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Embedding size:  100\n",
      "Number of heads:  4\n",
      "Head dimension:  25\n",
      "Dimension of Q, K and V, (batch_size, seq_len, number of head, head_dim):  torch.Size([2, 10, 4, 25])\n",
      "Dimension of scores: (batch_size, number of head, seq length, seq length),  torch.Size([2, 4, 10, 10])\n",
      "Dimension of attention weights: (batch size, number of head, seq_lenght, seq_length)  torch.Size([2, 4, 10, 10])\n",
      "Dimension of output (batch_size, number of head, seq length, head_dim) : torch.Size([2, 4, 10, 25])\n",
      "Dimension of output after projection (batch_size, seq_len, embed_dim) : torch.Size([2, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 200\n",
    "embed_dim = 100\n",
    "max_seq_len = 20\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "number_of_head = 4\n",
    "\n",
    "multihead = MultiHeadAttention(embed_dim,\n",
    "                              number_of_head\n",
    "                              )\n",
    "\n",
    "output_multihead = multihead(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9252bbb3-b222-4a2a-97d9-d5e6ac977fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 100])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_multihead.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f56343c-7ab1-4eef-88f4-38091549948e",
   "metadata": {},
   "source": [
    "#### **Add & Norm and Feed Forward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e21d483-f929-497b-8c89-017b168a8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer normalization is need to stop gradients from exploding or vanishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "861f610f-3896-4f2e-954e-da665c32d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_after_multihead = output + output_multihead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "956458ce-ebdf-4908-a965-68df55e823e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_after_multihead.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831f5b29-19dd-40cc-8836-673c3fb3cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim,\n",
    "                 ff_dim, dropout=0.1\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.Linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.Linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.Linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.Linear2(x)\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55955cab-3a29-4f7a-8ddf-337ac4ac8d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 200\n",
    "embed_dim = 100\n",
    "max_seq_len = 20\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "number_of_head = 4\n",
    "feedforward_dim = 50\n",
    "dropout = 0.1\n",
    "\n",
    "feedforward = FeedForward(embed_dim, feedforward_dim)\n",
    "feedforward_output = feedforward(output_after_multihead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7894d7d6-8ac3-4c83-8d25-fabc6d690963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56310a-70a6-48ed-9940-84ef6f761412",
   "metadata": {},
   "source": [
    "## **Decoder Section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9a12ce6-6ac6-4130-9e74-89d1a33dbbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = feedforward_output + pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac062446-bdbb-4eca-9bc7-239df8811538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHead(nn.Module):\n",
    "    def __init__(self, seq_length,\n",
    "                 embed_size, batch_size,\n",
    "                 number_of_head,\n",
    "                 max_seq_length,\n",
    "                 dropout = 0.1,\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert embed_dim % number_of_head == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = number_of_head\n",
    "        self.head_dim = embed_dim // number_of_head\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "        self.W_q = nn.Linear(self.embed_dim, \n",
    "                             self.embed_dim, \n",
    "                             bias=False\n",
    "                            )\n",
    "        self.W_k = nn.Linear(self.embed_dim,\n",
    "                             self.embed_dim,\n",
    "                             bias=False\n",
    "                            )\n",
    "        self.W_v = nn.Linear(self.embed_dim,\n",
    "                             self.embed_dim,\n",
    "                             bias=False\n",
    "                            )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(embed_dim,\n",
    "                                  embed_dim\n",
    "                                 )\n",
    "        self.register_buffer('causal_mask',\n",
    "                             torch.tril(\n",
    "                                 torch.ones(max_seq_length,\n",
    "                                            max_seq_length\n",
    "                                           )\n",
    "                             ))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, padding=False):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        #x = batch_size, seq_len, embed_dim and W = embed_dim x embed_dim\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.reshape(batch_size, seq_len,\n",
    "                    self.num_heads, self.head_dim\n",
    "                     )\n",
    "        K = K.reshape(batch_size, seq_len,\n",
    "                    self.num_heads, self.head_dim\n",
    "                     )\n",
    "        V = V.reshape(batch_size, seq_len,\n",
    "                    self.num_heads, self.head_dim\n",
    "                     )\n",
    "        \n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, \n",
    "                              #batch_size, number of head,\n",
    "                              #head_dim, seq_length\n",
    "                              K.transpose(-2, -1)\n",
    "                             )*self.scale\n",
    "        causal_mask = self.causal_mask[:seq_len, :seq_len]\n",
    "        scores = scores.masked_fill(causal_mask == 0,\n",
    "                                    float('-inf')\n",
    "                                   )\n",
    "        #output of scores = (batch_size, number_of_head,\n",
    "        #seq_length, seq_length\n",
    "\n",
    "        print('Dimension of scores: (batch_size, number of head, seq length, seq length), ', \n",
    "              scores.size()\n",
    "             )\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        #output of attention_weights = (batch_size,\n",
    "        #number_of_head, seq_length, seq_length)\n",
    "        print('Dimension of attention weights: (batch size, number of head, seq_lenght, seq_length) ',\n",
    "              attention_weights.size()\n",
    "             )\n",
    "\n",
    "        output = torch.matmul(attention_weights,\n",
    "                              V\n",
    "                             )\n",
    "        #(batch_size, number_of_head,\n",
    "        #seq_length, head_dim)\n",
    "        print('Dimension of output (batch_size, number of head, seq length, head_dim) :',\n",
    "              output.size()\n",
    "             )\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size,\n",
    "                             seq_len,\n",
    "                             embed_dim\n",
    "                            )\n",
    "        print('Dimension of output after projection (batch_size, seq_len, embed_dim) :',\n",
    "              output.size()\n",
    "             )\n",
    "        \n",
    "        return self.out_proj(output), attention_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e952df1-a9ec-41f8-a5e5-1e4991132fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of scores: (batch_size, number of head, seq length, seq length),  torch.Size([2, 4, 10, 10])\n",
      "Dimension of attention weights: (batch size, number of head, seq_lenght, seq_length)  torch.Size([2, 4, 10, 10])\n",
      "Dimension of output (batch_size, number of head, seq length, head_dim) : torch.Size([2, 4, 10, 25])\n",
      "Dimension of output after projection (batch_size, seq_len, embed_dim) : torch.Size([2, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 200\n",
    "embed_dim = 100\n",
    "max_seq_len = 20\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "number_of_head = 4\n",
    "feedforward_dim = 50\n",
    "dropout = 0.1\n",
    "\n",
    "masked_attention = MaskedMultiHead(seq_length=seq_len,\n",
    "                                   embed_size=embed_dim, \n",
    "                                   batch_size=batch_size,\n",
    "                                   number_of_head=number_of_head,\n",
    "                                   max_seq_length=max_seq_len,\n",
    "                                   dropout = dropout\n",
    "                                  )\n",
    "masked_output, attention_weights = masked_attention(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43ac1a98-18e1-484c-82c1-dad5795caada",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100\n",
    "layernorm = nn.LayerNorm(embed_dim)\n",
    "layernorm_output = layernorm(residual + masked_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55fa18-1639-400d-91d5-ee9f79f2025c",
   "metadata": {},
   "source": [
    "#### **Decoder Multihead Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4abfac3e-4bb5-44da-986c-9a2c37a16d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query from decoder, key and value from encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d209a36-8e7b-4e05-a8d9-a1becee13fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim,\n",
    "                 num_heads, \n",
    "                 dropout=0.1\n",
    "                ):\n",
    "        super().__init__()\n",
    "        print(\"Embedding dimension: \", embed_dim)\n",
    "        print(\"Number of heads: \", num_heads)\n",
    "        \n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = 1.0/ math.sqrt(self.head_dim)\n",
    "\n",
    "        self.W_q = nn.Linear(self.embed_dim, \n",
    "                             self.embed_dim, \n",
    "                             bias=False\n",
    "                            )\n",
    "        self.W_k = nn.Linear(self.embed_dim,\n",
    "                             self.embed_dim,\n",
    "                             bias=False\n",
    "                            )\n",
    "        self.W_v = nn.Linear(self.embed_dim,\n",
    "                             self.embed_dim,\n",
    "                             bias=False\n",
    "                            )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(embed_dim,\n",
    "                                  embed_dim\n",
    "                                 )\n",
    "        self.feedforward = nn.Linear(embed_dim,\n",
    "                                    embed_dim\n",
    "                                    )\n",
    "        self.layerNorm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value,\n",
    "                key_padding_mask=None\n",
    "               ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: \n",
    "            (batch_size, seq_len, embed_size)\n",
    "            - from decoder\n",
    "            key: (batch_size, seq_len, embed_size) \n",
    "            - from encoder\n",
    "            value: (batch_size, seq_len, embed_size)\n",
    "            - from encoder\n",
    "            \n",
    "        Returns:\n",
    "            output: (batch_size,seq_len, embed_size)\n",
    "            attention_weights: (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, query_len, embed_dim = query.shape\n",
    "        key_len = key.size(1)\n",
    "\n",
    "        Q = self.W_q(query)\n",
    "        #x = batch_size, seq_len, embed_dim and W = embed_dim x embed_dim\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "\n",
    "        Q = Q.reshape(batch_size, query_len,\n",
    "                    self.num_heads, self.head_dim\n",
    "                     )\n",
    "        K = K.reshape(batch_size, key_len,\n",
    "                    self.num_heads, self.head_dim\n",
    "                     )\n",
    "        V = V.reshape(batch_size, key_len,\n",
    "                    self.num_heads, self.head_dim\n",
    "                     )\n",
    "        \n",
    "        Q = Q.transpose(1, 2)\n",
    "        # batch_size, self.num_heads, query_len, self.head_dim\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, \n",
    "                              K.transpose(-2, -1)\n",
    "                             )*self.scale\n",
    "        #batch_size, number of head,\n",
    "        #query_len, key_len\n",
    "\n",
    "        print('Dimension of scores: (batch_size, number of head, seq length, seq length), ', \n",
    "              scores.size()\n",
    "             )\n",
    "        \n",
    "        attention_weights = F.softmax(scores, \n",
    "                                      dim=-1\n",
    "                                     )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        #output of attention_weights = (batch_size,\n",
    "        #number_of_head, query_len, seq_length)\n",
    "        print('Dimension of attention weights: (batch size, number of head, seq_lenght, seq_length) ',\n",
    "              attention_weights.size()\n",
    "             )\n",
    "\n",
    "        output = torch.matmul(attention_weights,\n",
    "                              V\n",
    "                             )\n",
    "        #(batch_size, number_of_head,\n",
    "        #seq_length, head_dim)\n",
    "        print('Dimension of output (batch_size, number of head, seq length, head_dim) :',\n",
    "              output.size()\n",
    "             )\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size,\n",
    "                             seq_len,\n",
    "                             embed_dim\n",
    "                            )\n",
    "        print('Dimension of output after projection (batch_size, seq_len, embed_dim) :',\n",
    "              output.size()\n",
    "             )\n",
    "\n",
    "        return self.layerNorm(self.feedforward(self.out_proj(output) + query)), attention_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a563dcba-e0f8-454d-bfc4-b58f418eab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension:  100\n",
      "Number of heads:  4\n",
      "Dimension of scores: (batch_size, number of head, seq length, seq length),  torch.Size([2, 4, 10, 10])\n",
      "Dimension of attention weights: (batch size, number of head, seq_lenght, seq_length)  torch.Size([2, 4, 10, 10])\n",
      "Dimension of output (batch_size, number of head, seq length, head_dim) : torch.Size([2, 4, 10, 25])\n",
      "Dimension of output after projection (batch_size, seq_len, embed_dim) : torch.Size([2, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 200\n",
    "embed_dim = 100\n",
    "max_seq_len = 20\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "number_of_head = 4\n",
    "feedforward_dim = 50\n",
    "dropout = 0.1\n",
    "\n",
    "decoder_cross_attn = DecoderCrossAttention(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=number_of_head,\n",
    "    dropout=0.1\n",
    ")\n",
    "decoder_cross_attn_output, attn_cross = decoder_cross_attn(\n",
    "    query=layernorm_output, \n",
    "    key=feedforward_output, \n",
    "    value=feedforward_output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36178548-d9ae-4dc2-8502-394929ac5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_cross_attn_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd4aea4c-f0c1-455a-a38f-4e6b1a31c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 200\n",
    "embed_dim = 100\n",
    "max_seq_len = 20\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "number_of_head = 4\n",
    "feedforward_dim = 50\n",
    "dropout = 0.1\n",
    "\n",
    "ff = FeedForward(embed_dim, feedforward_dim)\n",
    "ff_output = ff(decoder_cross_attn_output) #batch_size, seq_len, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8029202c-d736-4e7b-b273-a30e2a446676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSoftmax(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.linear = nn.Linear(embed_dim,\n",
    "                                vocab_size,\n",
    "                                bias=True\n",
    "                               )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        self.softmax = F.softmax(x, dim=-1)\n",
    "\n",
    "        return self.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33ec71ad-98bb-495d-aac1-4aa98f9d53de",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m feedforward_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      8\u001b[0m dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m---> 10\u001b[0m linear \u001b[38;5;241m=\u001b[39m \u001b[43mLinearSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m logits \u001b[38;5;241m=\u001b[39m linear(ff_output)\n",
      "Cell \u001b[0;32mIn[68], line 5\u001b[0m, in \u001b[0;36mLinearSoftmax.__init__\u001b[0;34m(self, embed_dim, vocab_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m vocab_size\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim,\n\u001b[1;32m      6\u001b[0m                         vocab_size,\n\u001b[1;32m      7\u001b[0m                         bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m                        )\n",
      "File \u001b[0;32m/N/slate/potem/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1968\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Module):\n\u001b[1;32m   1967\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1968\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1969\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign module before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1970\u001b[0m         )\n\u001b[1;32m   1971\u001b[0m     remove_from(\n\u001b[1;32m   1972\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m,\n\u001b[1;32m   1973\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters,\n\u001b[1;32m   1974\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers,\n\u001b[1;32m   1975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set,\n\u001b[1;32m   1976\u001b[0m     )\n\u001b[1;32m   1977\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m _global_module_registration_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "vocab_size = 200\n",
    "embed_dim = 100\n",
    "max_seq_len = 20\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "number_of_head = 4\n",
    "feedforward_dim = 50\n",
    "dropout = 0.1\n",
    "\n",
    "linear = LinearSoftmax(embed_dim=embed_dim,\n",
    "                       vocab_size=vocab_size\n",
    "                      )\n",
    "logits = linear(ff_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06467976-15c3-40bb-ac9e-c90d58cff8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
